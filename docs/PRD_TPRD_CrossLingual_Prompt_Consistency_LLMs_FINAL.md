# Evaluating Cross-Lingual Prompt Consistency in Large Language Models
**Version:** 2.0 (Post-Implementation)  
**Date:** 17 January 2026  
**Owner:** Baran Akin, Mehdi Farzin  
**Supervisor:** Shan Faiz  
**Decision Log:** Local-first execution (Ollama) to avoid request limits; Open-source LLMs only (no commercial baseline); **Final Models:** `gemma3:1b`, `gemma3:4b`, `llama3.2:1b`, `llama3.2:3b`, `phi3:latest`, `phi4-mini:3.8b`; LaBSE embeddings for open-text tasks only; Discrete-answer tasks evaluated via exact/normalized match; `temperature = 0.3`; `max_new_tokens = 256`; Two runs per prompt-language; Cross-lingual comparisons are paired by the same `run_id`; Intra-language stability baseline is measured as run1 vs run2 within each language.

---

## IMPLEMENTATION STATUS SUMMARY

> **Bu bÃ¶lÃ¼m, projenin gerÃ§ek uygulamasÄ± ile orijinal PRD arasÄ±ndaki farklÄ±lÄ±klarÄ± belgelemektedir.**

### âœ… BaÅŸarÄ±yla Tamamlanan Hedefler

| Hedef | Durum | Notlar |
|-------|-------|--------|
| G1: Cross-lingual tutarlÄ±lÄ±k Ã¶lÃ§Ã¼mÃ¼ | âœ… TamamlandÄ± | 288 karÅŸÄ±laÅŸtÄ±rma yapÄ±ldÄ± |
| G2: GÃ¶rev tÃ¼rÃ¼ analizi | âœ… TamamlandÄ± | 5 gÃ¶rev tÃ¼rÃ¼ analiz edildi |
| G3: Tekrarlanabilir metrikler | âœ… TamamlandÄ± | 31 gÃ¶rselleÅŸtirme Ã¼retildi |
| G4: Niteliksel kanÄ±t seti | âœ… TamamlandÄ± | 29 Ã¶rnek flaglendi |

### ğŸ“ Orijinal Plandan Sapmalar

#### 1. Model Listesi DeÄŸiÅŸiklikleri

**Orijinal Plan:**
- `gemma3:1b`
- `llama3.2:1b`

**GerÃ§ek Uygulama:**
- `gemma3:1b` âœ…
- `gemma3:4b` âœ… (eklendi)
- `llama3.2:1b` âœ…
- `llama3.2:3b` âœ… (eklendi)
- `phi3:latest` âœ… (eklendi)
- `phi4-mini:3.8b` âœ… (eklendi)
- ~~`qwen3:4b`~~ âŒ (kaldÄ±rÄ±ldÄ± - boÅŸ yanÄ±t sorunu)

**Sebep:** Daha kapsamlÄ± model karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in model Ã§eÅŸitliliÄŸi artÄ±rÄ±ldÄ±. qwen3:4b, 93/120 boÅŸ yanÄ±t Ã¼rettiÄŸi iÃ§in kaldÄ±rÄ±ldÄ±.

#### 2. Non-Compliance Detection DeÄŸiÅŸiklikleri

**Orijinal Plan:**
- Basit format kontrolÃ¼

**GerÃ§ek Uygulama (iteratif geliÅŸtirme):**
1. **Ä°lk versiyon:** Ã‡ok katÄ± (84 non-compliant)
2. **DÃ¼zeltme 1:** Uzunluk eÅŸiÄŸi 100â†’200
3. **DÃ¼zeltme 2:** Ã‡ok dilli etiket desteÄŸi (DE: positiv/negativ, TR: olumlu/olumsuz)
4. **DÃ¼zeltme 3:** A/B/C eÅŸleÅŸme esnekliÄŸi
5. **Final versiyon:** Sadece format kontrolÃ¼, doÄŸruluk kontrolÃ¼ kaldÄ±rÄ±ldÄ±

**SonuÃ§:** 84 â†’ 1 non-compliant (phi3:latest, TR, Prompt 7)

#### 3. Retry MekanizmasÄ± Eklendi

**Orijinal Plan:**
- Retry mekanizmasÄ± opsiyonel

**GerÃ§ek Uygulama:**
- Non-compliant yanÄ±tlar iÃ§in stricter prompt wrapper ile retry
- Retry sonrasÄ± hala non-compliant olanlar qualitative review iÃ§in saklandÄ±

#### 4. Ek GÃ¶rselleÅŸtirmeler

**Orijinal Plan:**
- Heatmap + Distribution plot

**GerÃ§ek Uygulama (31 dosya):**
- Model comparison (1)
- Heatmaps per model (6)
- Distributions per model (6)
- Task comparisons per model (6)
- Stability plots per model (6)
- Discrete summaries per model (6)

#### 5. Drift Type Kategorileri Eklendi

**Orijinal Plan:**
- Basit "low similarity" flagging

**GerÃ§ek Uygulama:**
- 5 drift type tanÄ±mlandÄ±:
  1. Semantic Drift
  2. Format Drift
  3. Factual Drift
  4. Style Drift
  5. Hallucination

### ğŸ“Š Uygulama Metrikleri

| Metrik | Plan | GerÃ§ek | Durum |
|--------|------|--------|-------|
| Toplam yanÄ±t | 120 Ã— M | 720 | âœ… (M=6) |
| Coverage | %100 | %100 | âœ… |
| Non-compliant rate | <%5 | %0.14 | âœ… |
| GÃ¶rselleÅŸtirme | 2+ | 31 | âœ… |
| Flagged Ã¶rnek | 6+ | 29 | âœ… |

### ğŸ”§ Teknik Zorluklar ve Ã‡Ã¶zÃ¼mleri

| Zorluk | Ã‡Ã¶zÃ¼m |
|--------|-------|
| CSV parsing hatasÄ± (German quotes) | `quoting=csv.QUOTE_ALL` ile yeniden Ã¼retim |
| qwen3:4b boÅŸ yanÄ±tlarÄ± | Model ve verisi kaldÄ±rÄ±ldÄ± |
| KatÄ± non-compliance tespiti | Ä°teratif refinement, Ã§ok dilli destek |
| TÃ¼rkÃ§e etiket tanÄ±ma | olumlu/olumsuz, positiv/negativ eklendi |

---

## 1) Repository Skeleton (proposed)

```text
crosslingual-consistency/
â”œâ”€ README.md
â”œâ”€ LICENSE
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â”œâ”€ configs/
â”‚  â”œâ”€ models.yaml               # model ids + decoding params
â”‚  â””â”€ embeddings.yaml           # LaBSE config
â”œâ”€ data/
â”‚  â”œâ”€ prompts_primary.csv       # 20 prompts x 3 languages (EN/DE/TR)
â”‚  â”œâ”€ responses.jsonl           # generated outputs + metadata (cached)
â”‚  â”œâ”€ metrics.csv               # cross-lingual semantic similarity for open-text tasks
â”‚  â”œâ”€ stability.csv            # intra-language stability (run1 vs run2) for open-text tasks
â”‚  â””â”€ task_metrics.csv          # task-aware agreement checks (where applicable)
â”œâ”€ src/
â”‚  â”œâ”€ load_prompts.py
â”‚  â”œâ”€ infer_ollama.py           # local inference client
â”‚  â”œâ”€ embed_labse.py            # LaBSE embedding
â”‚  â”œâ”€ similarity.py             # cosine + aggregation
â”‚  â”œâ”€ task_checks.py            # classification/reasoning/factual agreement
â”‚  â””â”€ plots.py                  # heatmap + distribution plots
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_collect_responses.ipynb
â”‚  â”œâ”€ 02_metrics_and_plots.ipynb
â”‚  â””â”€ 03_qualitative_review.ipynb
â””â”€ outputs/
   â”œâ”€ plots/
   â””â”€ reports/
```

---

## 2) Product Requirements Document (PRD)

### 2.1 Overview
**Problem:** LLMs often deliver strong English outputs, but can produce inconsistent answers in other languages for semantically equivalent prompts. This creates reliability and fairness risks for multilingual users.  
**Solution:** A controlled and reproducible evaluation framework that measures cross-lingual prompt consistency for English, German, and Turkish using (1) LaBSE-based semantic similarity and (2) task-aware agreement checks for discrete-answer tasks.

### 2.2 Goals / Non-Goals
**Goals**
- G1: Quantify cross-lingual semantic consistency across EN/DE/TR for a curated prompt set.
- G2: Identify which task types exhibit higher inconsistency (summarization, classification, reasoning, factual recall, creative).
- G3: Produce reproducible metrics and visualizations suitable for a proposal and an implementation-grade experiment.
- G4: Provide a short qualitative evidence set for semantic drift cases (low-consistency samples).

**Non-Goals**
- N1: Fine-tuning, RLHF, or modifying model weights.
- N2: Large-scale benchmarking across many languages beyond EN/DE/TR.
- N4: Building a production system or user-facing application beyond the evaluation pipeline.

### 2.3 Personas & Use Cases
- **Research Student:** Needs a defensible methodology and reproducible results for a short academic project.
- **ML Engineer:** Wants a lightweight way to compare multilingual stability across open LLMs.
- **Instructor/Reviewer:** Needs transparent assumptions, datasets, and evaluation steps.

Use cases:
- Compare two open-source instruction-tuned models for multilingual consistency.
- Detect which task types produce higher semantic drift for Turkish vs German.
- Generate an interpretable report (plots + flagged examples) from one run.

### 2.4 Scope (what is evaluated)
- Languages: EN, DE, TR
- Models: open-source instruction-tuned LLMs (local-first)
- Prompt dataset: 20 items, 5 task types
- Runs: two runs per prompt-language condition
- Metrics:
  - Primary (open-text tasks: summarization, creative): LaBSE embeddings + cosine similarity (paired EN-DE, EN-TR, DE-TR by the same `run_id`)
  - Primary (discrete-answer tasks: classification, reasoning, factual): exact/normalized match via task-aware checks
  - Secondary (optional): LaBSE cosine for discrete-answer tasks as an exploratory signal only (reported separately)
- Outputs:
  - response archive (JSONL)
  - metrics tables (CSV)
  - plots (heatmap + distributions)
  - small qualitative set (flagged lowest similarity examples)

### 2.5 Success Metrics & KPIs (student project targets)
- **Coverage:** 20 prompts x 3 languages x 2 runs per selected model completed without missing cells.
- **Reproducibility:** Prompts, parameters, and results are logged and re-runnable.
- **Interpretability:** At least:
  - 1 heatmap per model
  - 1 distribution plot per model
  - 1 table summarizing mean/std by language pair and task type
- **Qualitative evidence:** At least 6 flagged cases (lowest similarity) with short annotation of drift type.

### 2.6 Key Decisions

- **LLM access:** local-first via Ollama to avoid API request limits and ensure repeatability.
- **Embedding encoder:** LaBSE only (used as the semantic metric for open-text tasks).
- **Discrete-answer evaluation:** exact/normalized match (task-aware checks) is the primary metric for classification, reasoning, and factual prompts.
- **Decoding:** `temperature = 0.3`, `max_new_tokens = 256`.
- **Runs:** 2 runs per prompt-language.
- **Run pairing (cross-lingual):** compare EN/DE/TR outputs generated with the same `run_id` (paired comparisons: run1-to-run1, run2-to-run2).
- **Stability baseline (intra-language):** compare run1 vs run2 within EN, within DE, and within TR to quantify model randomness independent of translation.
- **Public datasets:** optional extension only (not required for completion).

### 2.7 Risks & Mitigations
- **Translation drift creates fake inconsistency:** Manual translation plus back-translation checks; revise items with meaning shift.
- **Cosine similarity reflects semantics, not correctness (and can be unstable for very short outputs):** Use LaBSE cosine as the primary metric only for open-text tasks. For discrete-answer tasks (classification/reasoning/factual), use exact/normalized match as the primary metric and treat any embedding-based score as exploratory.
- **Length differences distort embeddings:** Limit output length via `max_new_tokens = 256` and concise prompt instructions.
- **Compute constraints for local models:** Keep dataset small; cache outputs in JSONL; prioritize one model if needed.

### 2.8 Acceptance Criteria (PRD)
- A: Primary dataset is available as CSV with prompt_id, task_type, language, and full prompt text.
- B: Response archive contains at least 120 outputs per model (20 prompts x 3 languages x 2 runs).
- B2: `stability.csv` contains intra-language stability results (run1 vs run2) for EN, DE, and TR, for open-text and discrete-answer tasks.
- C: For open-text tasks (summarization, creative), `metrics.csv` contains paired cross-lingual cosine similarities for all prompt ids and language pairs (EN-DE, EN-TR, DE-TR), for both runs.
- D: For discrete-answer tasks (classification, reasoning, factual), `task_metrics.csv` contains paired cross-lingual match/mismatch results and is included in the summary reporting.
- E: Plots are generated and saved (heatmap + distribution, plus a stability context view).
- F: A flagged qualitative set of low-consistency examples is produced for review.

---

## 3) Technical PRD (TPRD)

### 3.1 Architecture (single-pass pipeline)
```
prompts_primary.csv
  -> inference (Ollama local, fixed decoding params)
  -> responses.jsonl (canonical cache)
  -> task routing by task_type
     - open-text (summarization, creative): LaBSE embeddings -> cosine similarity (paired EN-DE, EN-TR, DE-TR by the same run_id)
     - discrete-answer (classification, reasoning, factual): task-aware extraction -> exact/normalized match
  -> intra-language stability baseline
     - open-text: LaBSE cosine for run1 vs run2 within EN/DE/TR
     - discrete-answer: match rate for run1 vs run2 within EN/DE/TR
  -> aggregation (by pair, by task type, and vs stability baseline)
  -> plots (heatmap + distributions)
  -> outputs/ (metrics tables + plots + flagged examples)
```

### 3.2 Data Schemas

**Prompt dataset schema (CSV)**
- `prompt_id` (int)
- `task_type` (summarization | classification | reasoning | factual | creative)
- `language` (EN | DE | TR)
- `text` (string, full instruction plus input)

**Response dataset schema (JSONL)**
- Note: include `non_compliant` boolean for output-format violations.
```json
{
  "prompt_id": 1,
  "task_type": "summarization",
  "language": "EN",
  "model_id": "llama3.2:1b",
  "run_id": 1,
  "temperature": 0.3,
  "max_new_tokens": 256,
  "timestamp_utc": "2026-01-17T12:00:00Z",
  "prompt_text": "...",
  "response_text": "...",
  "non_compliant": false
}
```

**Metrics schema (CSV: `metrics.csv`, open-text only)**
- `model_id`
- `prompt_id`
- `task_type` (summarization | creative)
- `pair` (EN-DE | EN-TR | DE-TR)
- `run_id` (paired cross-lingual comparison: run1-to-run1, run2-to-run2)
- `cosine_similarity`
- `flag_low_similarity` (bool, bottom 10% within model)

**Stability schema (CSV: `stability.csv`)**
- `model_id`
- `prompt_id`
- `task_type`
- `language` (EN | DE | TR)
- `stability_type` (open_text_cosine | discrete_match)
- `run_id_a` (1)
- `run_id_b` (2)
- `stability_value` (cosine similarity for open-text; 1/0 match for discrete-answer tasks)

**Task-aware metrics schema (CSV)**
- `model_id`
- `prompt_id`
- `task_type`
- `check_type` (classification | reasoning | factual)
- `result` (match | mismatch | uncertain)
- `key_en` / `key_de` / `key_tr` (extracted where applicable)

### 3.3 Model Inference Requirements (Ollama)
**Response Language Policy**
- For EN prompts: respond in English.
- For DE prompts: respond in German.
- For TR prompts: respond in Turkish.
- For tasks that require fixed tokens (e.g., classification labels, A/B/C multiple-choice, numeric-only answers): the output format requirement takes precedence; extra commentary is not allowed.

**Prompt Wrapper (applied programmatically per request)**
Prepend the following control line to every prompt:
- EN: `Respond in English. Follow the output format exactly. Do not add explanations.`
- DE: `Antworte auf Deutsch. Halte dich exakt an das Ausgabeformat. Keine ErklÃ¤rungen.`
- TR: `TÃ¼rkÃ§e yanÄ±t ver. Ã‡Ä±ktÄ± formatÄ±na tam uy. AÃ§Ä±klama ekleme.`

**Ollama parameter mapping**
- `max_new_tokens` corresponds to `num_predict` in Ollama.
- Keep other sampling parameters at defaults unless explicitly set; log the effective parameters used.

**Models (final implementation, Ollama tags)**
- `gemma3:1b` - Google, 1B parameters
- `gemma3:4b` - Google, 4B parameters
- `llama3.2:1b` - Meta, 1B parameters
- `llama3.2:3b` - Meta, 3B parameters
- `phi3:latest` - Microsoft, 3.8B parameters
- `phi4-mini:3.8b` - Microsoft, 3.8B parameters
- ~~`qwen3:4b`~~ - Removed (empty response issues)

**Fixed decoding parameters**
- `temperature = 0.3`
- `max_new_tokens = 256`
- Two runs per prompt-language, stored as separate `run_id` records.

**Request volume**
For M models, total inference calls:
- Calls = 20 prompts x 3 languages x 2 runs x M
- Calls = 120 x M

**Actual implementation (M=6):**
- Total calls = 120 x 6 = **720 responses**
- Inference time: ~8 minutes (local Ollama)

Optional public dataset extension adds:
- Calls_additional = N x 3 x 2 x M = 6N x M
Where N is the number of added public items (EN/DE/TR triplets).

**Note:** qwen3:4b was initially included (M=7) but removed after 93/120 empty responses.

### 3.4 Embedding and Similarity (LaBSE-only, open-text tasks)
**Applicability**
- Use LaBSE cosine as the semantic metric only for open-text outputs (summarization, creative).
- For discrete-answer tasks (classification, reasoning, factual), treat LaBSE cosine as exploratory only and do not mix it into the main aggregate.

**Encoder**
- `sentence-transformers/LaBSE`

**Cross-lingual computation (paired by run_id)**
- Embed each open-text response as a dense vector.
- Compute cosine similarity for each prompt and each language pair using the same `run_id` across languages:
  - run1: EN_run1 vs DE_run1, EN_run1 vs TR_run1, DE_run1 vs TR_run1
  - run2: EN_run2 vs DE_run2, EN_run2 vs TR_run2, DE_run2 vs TR_run2

**Intra-language stability baseline**
- For open-text tasks, compute cosine similarity of run1 vs run2 within each language (EN, DE, TR) and store it in `stability.csv`.

**Aggregation**
- Mean and standard deviation by language pair (cross-lingual).
- Mean and standard deviation by task type (open-text only).
- Compare cross-lingual consistency against the intra-language stability baseline to separate language effects from sampling randomness.

**Flagging**
- Mark the bottom 10% similarity values per model as `flag_low_similarity = True` for qualitative inspection.

### 3.5 Task-aware checks (primary for discrete-answer tasks)
These checks are the primary evaluation signal for classification, reasoning, and factual prompts, because outputs are intentionally short and format-constrained (e.g., `A/B/C`, years, formulas), where embedding cosine can be unstable.

**Classification prompts**
- Extract predicted label (positive/negative, agreement/disagreement, request/complaint, formal/informal).
- Cross-lingual: check whether EN/DE/TR labels match for the same `run_id`.
- Intra-language stability: check whether run1 label equals run2 label within each language; store 1/0 in `stability.csv` with `stability_type = discrete_match`.

**Reasoning prompts**
- Extract numeric or deterministic answer (speed, duration, ordering).
- Cross-lingual: check whether answers match across languages for the same `run_id`.
- Intra-language stability: run1 vs run2 within each language (1/0).

**Factual prompts**
- Extract key entity (inventor, year, capital, author).
- Cross-lingual: check whether extracted entities match across languages for the same `run_id`.
- Intra-language stability: run1 vs run2 within each language (1/0).

**Outputs**
- Write cross-lingual match/mismatch results to `task_metrics.csv`.
- Write intra-language stability results (1/0 for discrete tasks) to `stability.csv`.

### 3.6 Visualizations (required)
- **Heatmap:** rows prompt_id, columns language pairs, values cosine similarity.
- **Distribution plot:** boxplot or density per language pair.
- **Summary table:** mean/std by pair and by task type (open-text), plus discrete match rates (classification/reasoning/factual).
- **Stability table/plot:** run1 vs run2 stability by language (EN/DE/TR) to contextualize cross-lingual results.

### 3.7 Reproducibility and Logging
- Log: model_id, decoding params, prompt_id, language, run_id, timestamp.
- Capture environment versions: Ollama version, model tag + digest, Python version, and `sentence-transformers` version.
- `responses.jsonl` is the canonical cache and should be treated as immutable once generated.
- Store config files used for the run (`models.yaml`, `embeddings.yaml`) alongside outputs.

**Output compliance handling**
- Validate each response against the expected output format.
- If a response violates constraints (e.g., adds explanations when â€œoutput onlyâ€ is required):
  - Record `non_compliant = true` in the response metadata.
  - Optionally re-run the same prompt once with the same parameters and a stricter wrapper line.
  - If still non-compliant, keep the original and exclude from exact-match task checks (but keep for qualitative review).


### 3.8 One-shot Execution Checklist (no sprints)
1. Finalize prompts and translations. Run back-translation checks and revise if needed.
2. Generate responses for all models and runs. Save to `responses.jsonl`.
3. Compute intra-language stability baselines (run1 vs run2) and save to `stability.csv`.
4. Compute LaBSE embeddings and paired cross-lingual cosine similarities for open-text tasks. Save to `metrics.csv`.
5. Compute task-aware cross-lingual checks for discrete-answer tasks. Save to `task_metrics.csv`.
6. Generate plots and summary tables (including stability context). Save to `outputs/plots/` and `outputs/reports/`.
7. Create a flagged qualitative set from the lowest similarity cases.

### 3.9 Acceptance (TPRD)
- `/data/prompts_primary.csv` exists with 20 items x 3 languages.
- `responses.jsonl` contains 120 x M records, with correct metadata and no missing languages.
- `stability.csv` includes intra-language stability results (run1 vs run2) for EN/DE/TR.
- `metrics.csv` includes paired cross-lingual cosine scores for open-text tasks (summarization/creative) and all language pairs.
- `task_metrics.csv` includes task-aware checks for applicable prompts.
- Plots are generated and saved.
- Flagged examples list exists and can be reviewed.

---

## 10) Timeline (Milestones)

**M1 â€” Dataset readiness**
- Finalize the 20-item primary prompt set (EN source).
- Manual translation to DE and TR.
- Back-translation validation and revision of any drifting items.
- Output: `data/prompts_primary.csv` (complete EN/DE/TR triplets).

**M2 â€” Inference run completed (local-first)**
- Configure Ollama models and fixed decoding parameters (`temperature=0.3`, `max_new_tokens=256`).
- Execute two runs per prompt-language per model.
- Output: `data/responses.jsonl` (canonical cache, complete coverage).

**M3 â€” Stability baseline + semantic similarity metrics**
- Compute intra-language stability (run1 vs run2) within EN, DE, TR:
  - open-text: LaBSE cosine
  - discrete-answer: match rate (1/0)
- Compute LaBSE embeddings for open-text responses.
- Calculate paired cross-lingual cosine similarity for ENâ€“DE, ENâ€“TR, and DEâ€“TR (same run_id across languages).
- Aggregate mean/std by language pair and task type (open-text), and report discrete match rates separately.
- Outputs: `data/stability.csv`, `data/metrics.csv`.

**M4 â€” Task-aware cross-lingual checks completed (discrete-answer tasks)**
- Run lightweight agreement checks for discrete-answer prompts (paired by run_id):
  - classification label agreement
  - reasoning numeric/deterministic agreement
  - factual key-entity agreement
- Output: `data/task_metrics.csv`.

**M5 â€” Visualization and review package**
- Generate plots: heatmap + distribution plot per model.
- Produce a flagged set of lowest-similarity cases for manual inspection.
- Output: `outputs/plots/*` + `outputs/reports/*`.

---

## Appendix A) Primary Prompt and Input Set (20 items, EN/DE/TR)

### Summarization (1 to 4)

**1.**  
- EN: Summarize the following paragraph in one sentence: â€œArtificial intelligence is changing many industries by automating repetitive work and helping people make faster decisions. Companies use AI to detect patterns in large datasets, but the results depend on data quality and careful evaluation. While productivity can increase, some tasks may be replaced and employees may need reskilling. Clear policies are also needed to reduce privacy risks and unfair outcomes.â€  
- DE: Fasse den folgenden Absatz in einem Satz zusammen: â€KÃ¼nstliche Intelligenz verÃ¤ndert viele Branchen, indem sie repetitive Arbeit automatisiert und Menschen hilft, schneller Entscheidungen zu treffen. Unternehmen nutzen KI, um Muster in groÃŸen DatensÃ¤tzen zu erkennen, aber die Ergebnisse hÃ¤ngen von DatenqualitÃ¤t und sorgfÃ¤ltiger Bewertung ab. Obwohl die ProduktivitÃ¤t steigen kann, kÃ¶nnen einige TÃ¤tigkeiten ersetzt werden und BeschÃ¤ftigte benÃ¶tigen mÃ¶glicherweise Umschulungen. AuÃŸerdem sind klare Richtlinien nÃ¶tig, um Datenschutzrisiken und unfairen Ergebnissen vorzubeugen.â€œ  
- TR: AÅŸaÄŸÄ±daki paragrafÄ± tek cÃ¼mlede Ã¶zetle: â€œYapay zeka, tekrarlayan iÅŸleri otomatikleÅŸtirerek ve daha hÄ±zlÄ± karar vermeye yardÄ±mcÄ± olarak birÃ§ok sektÃ¶rÃ¼ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor. Åirketler bÃ¼yÃ¼k veri kÃ¼melerinde Ã¶rÃ¼ntÃ¼leri yakalamak iÃ§in yapay zekayÄ± kullanÄ±yor, ancak sonuÃ§lar veri kalitesine ve dikkatli deÄŸerlendirmeye baÄŸlÄ±dÄ±r. Verimlilik artabilse de bazÄ± gÃ¶revler ortadan kalkabilir ve Ã§alÄ±ÅŸanlarÄ±n yeniden beceri kazanmasÄ± gerekebilir. AyrÄ±ca gizlilik risklerini ve adaletsiz sonuÃ§larÄ± azaltmak iÃ§in net politikalar gerekir.â€

**2.**  
- EN: Summarize the following text in one sentence: â€œClimate change is contributing to rising sea levels and more frequent extreme weather events in many regions. Coastal cities face higher flood risk, and storms can damage infrastructure and disrupt supply chains. Adaptation measures such as flood barriers and heat plans can reduce harm, but they require long-term funding. Reducing emissions remains essential to limit future impacts.â€  
- DE: Fasse diesen Text in einem Satz zusammen: â€Der Klimawandel trÃ¤gt in vielen Regionen zu steigenden Meeresspiegeln und hÃ¤ufigeren Extremwetterereignissen bei. KÃ¼stenstÃ¤dte sind stÃ¤rker von Ãœberschwemmungen bedroht, und StÃ¼rme kÃ¶nnen Infrastruktur beschÃ¤digen und Lieferketten stÃ¶ren. AnpassungsmaÃŸnahmen wie Hochwasserschutz und HitzeschutzplÃ¤ne kÃ¶nnen SchÃ¤den verringern, erfordern jedoch langfristige Finanzierung. Gleichzeitig bleibt die Reduktion von Emissionen entscheidend, um zukÃ¼nftige Folgen zu begrenzen.â€œ  
- TR: Bu metni tek cÃ¼mlede Ã¶zetle: â€œÄ°klim deÄŸiÅŸikliÄŸi, birÃ§ok bÃ¶lgede deniz seviyelerinin yÃ¼kselmesine ve aÅŸÄ±rÄ± hava olaylarÄ±nÄ±n daha sÄ±k gÃ¶rÃ¼lmesine katkÄ±da bulunuyor. KÄ±yÄ± ÅŸehirlerinde sel riski artÄ±yor ve fÄ±rtÄ±nalar altyapÄ±ya zarar vererek tedarik zincirlerini aksatabiliyor. Setler ve sÄ±cak hava planlarÄ± gibi uyum Ã¶nlemleri zararÄ± azaltabilir, ancak uzun vadeli finansman gerektirir. Gelecekteki etkileri sÄ±nÄ±rlamak iÃ§in emisyonlarÄ±n azaltÄ±lmasÄ± hÃ¢lÃ¢ kritik Ã¶nemdedir.â€

**3.**  
- EN: Summarize the following paragraph in one sentence: â€œElectric vehicles can reduce local air pollution because they have no tailpipe emissions, but their total climate benefit depends on how electricity is generated. Battery production requires mining and energy-intensive processing, which can create environmental impacts. Recycling and longer battery lifetimes can improve sustainability over time. Policy and infrastructure choices strongly influence whether the overall system becomes cleaner.â€  
- DE: Fasse den folgenden Absatz in einem Satz zusammen: â€Elektrofahrzeuge kÃ¶nnen die lokale Luftverschmutzung reduzieren, da sie keine Auspuffemissionen haben, aber ihr gesamter Klimavorteil hÃ¤ngt davon ab, wie Strom erzeugt wird. Die Batterieproduktion erfordert Rohstoffabbau und energieintensive Verarbeitung, was Umweltbelastungen verursachen kann. Recycling und lÃ¤ngere Batterielebensdauern kÃ¶nnen die Nachhaltigkeit mit der Zeit verbessern. Politische Entscheidungen und Infrastruktur beeinflussen stark, ob das Gesamtsystem sauberer wird.â€œ  
- TR: AÅŸaÄŸÄ±daki paragrafÄ± tek cÃ¼mlede Ã¶zetle: â€œElektrikli araÃ§lar egzoz emisyonu Ã¼retmedikleri iÃ§in yerel hava kirliliÄŸini azaltabilir, ancak toplam iklim faydasÄ± elektriÄŸin nasÄ±l Ã¼retildiÄŸine baÄŸlÄ±dÄ±r. Pil Ã¼retimi madencilik ve enerji yoÄŸun iÅŸlemler gerektirir ve bu da Ã§evresel etkiler yaratabilir. Geri dÃ¶nÃ¼ÅŸÃ¼m ve pillerin daha uzun Ã¶mÃ¼rlÃ¼ olmasÄ± zamanla sÃ¼rdÃ¼rÃ¼lebilirliÄŸi artÄ±rabilir. Politika ve altyapÄ± tercihleri, sistemin genel olarak daha temiz hale gelip gelmeyeceÄŸini gÃ¼Ã§lÃ¼ biÃ§imde belirler.â€

**4.**  
- EN: Summarize the following paragraph in one sentence: â€œRemote work can increase flexibility and reduce commuting time, which may improve well-being for some employees. However, it can also reduce informal collaboration and make onboarding harder for new team members. Communication often becomes more scheduled and documentation-heavy, which changes how teams coordinate. Clear boundaries and hybrid policies can help prevent burnout and maintain productivity.â€  
- DE: Fasse den folgenden Absatz in einem Satz zusammen: â€Remote-Arbeit kann die FlexibilitÃ¤t erhÃ¶hen und Pendelzeiten reduzieren, was das Wohlbefinden mancher BeschÃ¤ftigter verbessern kann. Gleichzeitig kann sie informelle Zusammenarbeit verringern und das Onboarding neuer Teammitglieder erschweren. Kommunikation wird hÃ¤ufig stÃ¤rker geplant und dokumentationslastiger, wodurch sich die Zusammenarbeit verÃ¤ndert. Klare Grenzen und Hybrid-Regelungen kÃ¶nnen helfen, Burnout zu vermeiden und die ProduktivitÃ¤t zu sichern.â€œ  
- TR: AÅŸaÄŸÄ±daki paragrafÄ± tek cÃ¼mlede Ã¶zetle: â€œUzaktan Ã§alÄ±ÅŸma esnekliÄŸi artÄ±rabilir ve iÅŸe gidiÅŸ geliÅŸ sÃ¼resini azaltarak bazÄ± Ã§alÄ±ÅŸanlarÄ±n iyi oluÅŸunu destekleyebilir. Ancak gayriresmi iÅŸ birliÄŸini azaltabilir ve yeni ekip Ã¼yelerinin iÅŸe uyumunu zorlaÅŸtÄ±rabilir. Ä°letiÅŸim daha planlÄ± ve dokÃ¼mantasyon aÄŸÄ±rlÄ±klÄ± hale gelir, bu da ekiplerin koordinasyon biÃ§imini deÄŸiÅŸtirir. Net sÄ±nÄ±rlar ve hibrit politikalar, tÃ¼kenmiÅŸliÄŸi Ã¶nleyip verimliliÄŸi korumaya yardÄ±mcÄ± olabilir.â€

### Classification (5 to 8)

**5. Sentiment (output must be exactly: Positive or Negative)**  
- EN: Label the sentiment as Positive or Negative: â€œAfter a week of use, the phone overheats during video calls, the battery drops from 100% to 20% by afternoon, and the screen shows random flickering. I regret buying it.â€  
- DE: Ordne die Stimmung als Positiv oder Negativ ein: â€Nach einer Woche Nutzung wird das Handy bei Videoanrufen sehr heiÃŸ, der Akku fÃ¤llt bis zum Nachmittag von 100% auf 20%, und der Bildschirm flackert zufÃ¤llig. Ich bereue den Kauf.â€œ  
- TR: Duyguyu Olumlu veya Olumsuz olarak etiketle: â€œBir haftalÄ±k kullanÄ±mÄ±n ardÄ±ndan telefon gÃ¶rÃ¼ntÃ¼lÃ¼ aramalarda aÅŸÄ±rÄ± Ä±sÄ±nÄ±yor, pil Ã¶ÄŸleden sonra 100%â€™den 20%â€™ye dÃ¼ÅŸÃ¼yor ve ekran rastgele titriyor. AldÄ±ÄŸÄ±ma piÅŸmanÄ±m.â€

**6. Agreement (output must be exactly: Agree or Disagree)**  
- EN: Decide if the response indicates agreement or disagreement. Output only Agree or Disagree: Statement: â€œPublic transport should be expanded to reduce traffic.â€ Response: â€œI see your point, but I donâ€™t think it would solve congestion in this city.â€  
- DE: Entscheide, ob die Antwort Zustimmung oder Ablehnung ausdrÃ¼ckt. Gib nur Agree oder Disagree aus: Aussage: â€Der Ã¶ffentliche Nahverkehr sollte ausgebaut werden, um den Verkehr zu reduzieren.â€œ Antwort: â€Ich verstehe deinen Punkt, aber ich glaube nicht, dass das hier die Staus wirklich lÃ¶st.â€œ  
- TR: YanÄ±tÄ±n katÄ±lÄ±m mÄ± karÅŸÄ± Ã§Ä±kma mÄ± olduÄŸunu belirle. Sadece Agree veya Disagree yaz: Ä°fade: â€œTrafiÄŸi azaltmak iÃ§in toplu taÅŸÄ±ma geniÅŸletilmeli.â€ YanÄ±t: â€œSÃ¶ylediÄŸini anlÄ±yorum ama bunun bu ÅŸehirde sÄ±kÄ±ÅŸÄ±klÄ±ÄŸÄ± Ã§Ã¶zeceÄŸini dÃ¼ÅŸÃ¼nmÃ¼yorum.â€

**7. Intent (output must be exactly: Request or Complaint)**  
- EN: Classify the primary intent as Request or Complaint. Output only Request or Complaint: â€œMy order arrived with a cracked screen protector and missing accessories. This is unacceptable. Please issue a refund or send a replacement.â€  
- DE: Ordne die Hauptabsicht als Request oder Complaint ein. Gib nur Request oder Complaint aus: â€Meine Bestellung kam mit einem gesprungenen Displayschutz und fehlendem ZubehÃ¶r an. Das ist inakzeptabel. Bitte erstatten Sie den Betrag oder senden Sie einen Ersatz.â€œ  
- TR: Ana niyeti Request veya Complaint olarak sÄ±nÄ±flandÄ±r. Sadece Request veya Complaint yaz: â€œSipariÅŸim ekran koruyucusu Ã§atlak ve aksesuarlarÄ± eksik ÅŸekilde geldi. Bu kabul edilemez. LÃ¼tfen para iadesi yapÄ±n veya yenisini gÃ¶nderin.â€

**8. Formality (output must be exactly: Formal or Informal)**  
- EN: Label the message as Formal or Informal. Output only Formal or Informal: â€œHi Alex, could you send me the updated document when you have a moment? Thanks!â€  
- DE: Markiere die Nachricht als Formal oder Informal. Gib nur Formal oder Informal aus: â€Hi Alex, kÃ¶nntest du mir das aktualisierte Dokument schicken, wenn du kurz Zeit hast? Danke!â€œ  
- TR: MesajÄ± Formal veya Informal olarak etiketle. Sadece Formal veya Informal yaz: â€œSelam Alex, mÃ¼sait olduÄŸunda gÃ¼ncellenmiÅŸ dokÃ¼manÄ± bana gÃ¶nderebilir misin? TeÅŸekkÃ¼rler!â€

### Reasoning (9 to 12)

**9. Logic (output must be exactly: A, B, or C)**  
- EN: If A is taller than B and B is taller than C, who is the tallest? Output only A, B, or C.  
- DE: Wenn A grÃ¶ÃŸer als B und B grÃ¶ÃŸer als C ist, wer ist am grÃ¶ÃŸten? Gib nur A, B oder C aus.  
- TR: A, Bâ€™den ve B de Câ€™den uzunsa, en uzun kimdir? Sadece A, B veya C yaz.

**10. Math (output must be a single number with unit: km/h)**  
- EN: A car travels 90 kilometers in 1.5 hours. What is its average speed? Output only the final value in km/h.  
- DE: Ein Auto fÃ¤hrt 90 Kilometer in 1,5 Stunden. Wie hoch ist die Durchschnittsgeschwindigkeit? Gib nur den Endwert in km/h aus.  
- TR: Bir araba 1,5 saatte 90 kilometre gidiyor. Ortalama hÄ±zÄ± nedir? Sadece km/h cinsinden nihai deÄŸeri yaz.

**11. Time (output must be a single value in minutes)**  
- EN: A meeting started at 9:15 and ended at 10:45. How many minutes did it last? Output only the number of minutes.  
- DE: Ein Meeting begann um 9:15 und endete um 10:45. Wie viele Minuten dauerte es? Gib nur die Minutenanzahl aus.  
- TR: ToplantÄ± 9:15â€™te baÅŸlayÄ±p 10:45â€™te bitti. KaÃ§ dakika sÃ¼rdÃ¼? Sadece dakika sayÄ±sÄ±nÄ± yaz.

**12. Decision (multiple-choice; output must be exactly: A, B, or C)**  
- EN: It is 2Â°C and raining. Which option is most appropriate to wear? A) T-shirt and sandals B) Warm waterproof jacket and closed shoes C) Shorts and sunglasses. Output only A, B, or C.  
- DE: Es sind 2Â°C und es regnet. Welche Option ist am geeignetsten? A) T-Shirt und Sandalen B) Warme wasserdichte Jacke und geschlossene Schuhe C) Shorts und Sonnenbrille. Gib nur A, B oder C aus.  
- TR: Hava 2Â°C ve yaÄŸmur yaÄŸÄ±yor. Hangisini giymek en uygundur? A) TiÅŸÃ¶rt ve sandalet B) SÄ±cak su geÃ§irmez mont ve kapalÄ± ayakkabÄ± C) Åort ve gÃ¼neÅŸ gÃ¶zlÃ¼ÄŸÃ¼. Sadece A, B veya C yaz.

### Factual Recall (13 to 16)

**13. (output must be exactly the formula)**  
- EN: What is the chemical formula for water? Output only the formula.  
- DE: Wie lautet die chemische Formel fÃ¼r Wasser? Gib nur die Formel aus.  
- TR: Suyun kimyasal formÃ¼lÃ¼ nedir? Sadece formÃ¼lÃ¼ yaz.

**14. (output must be exactly the year)**  
- EN: In which year did World War II end? Output only the year.  
- DE: In welchem Jahr endete der Zweite Weltkrieg? Gib nur das Jahr aus.  
- TR: Ä°kinci DÃ¼nya SavaÅŸÄ± hangi yÄ±lda sona erdi? Sadece yÄ±lÄ± yaz.

**15. (output must be exactly the city name)**  
- EN: What is the capital city of Canada? Output only the city name.  
- DE: Was ist die Hauptstadt von Kanada? Gib nur den StÃ¤dtenamen aus.  
- TR: Kanadaâ€™nÄ±n baÅŸkenti neresidir? Sadece ÅŸehir adÄ±nÄ± yaz.

**16. (output must be exactly the authorâ€™s name)**  
- EN: Who wrote the novel â€œ1984â€? Output only the authorâ€™s name.  
- DE: Wer schrieb den Roman â€1984â€œ? Gib nur den Namen des Autors aus.  
- TR: â€œ1984â€ romanÄ±nÄ± kim yazdÄ±? Sadece yazarÄ±n adÄ±nÄ± yaz.

### Creative Generation (17 to 20)

**17. Tagline (2 sentences; no bullet points)**  
- EN: Write exactly two sentences promoting teamwork in a workplace. Keep the tone professional. No bullet points.  
- DE: Schreibe genau zwei SÃ¤tze, die Teamarbeit am Arbeitsplatz bewerben. Professioneller Ton. Keine AufzÃ¤hlungspunkte.  
- TR: Ä°ÅŸ yerinde takÄ±m Ã§alÄ±ÅŸmasÄ±nÄ± teÅŸvik eden tam olarak iki cÃ¼mle yaz. Tonu profesyonel olsun. Madde iÅŸareti kullanma.

**18. Short ad copy (2 sentences; include one eco benefit + one taste note)**  
- EN: Write exactly two sentences of ad copy for an eco-friendly coffee brand. Include one environmental benefit and one taste description. No bullet points.  
- DE: Schreibe genau zwei WerbesÃ¤tze fÃ¼r eine umweltfreundliche Kaffeemarke. Nenne einen Umweltvorteil und eine Geschmacksbeschreibung. Keine AufzÃ¤hlungspunkte.  
- TR: Ã‡evre dostu bir kahve markasÄ± iÃ§in tam olarak iki cÃ¼mle reklam metni yaz. Bir Ã§evresel fayda ve bir tat aÃ§Ä±klamasÄ± ekle. Madde iÅŸareti kullanma.

**19. Slogan (6 to 8 words; output only the slogan)**  
- EN: Write a slogan for an online education platform. It must be 6 to 8 words. Output only the slogan.  
- DE: Schreibe einen Slogan fÃ¼r eine Online-Bildungsplattform. Er muss aus 6 bis 8 WÃ¶rtern bestehen. Gib nur den Slogan aus.  
- TR: Bir Ã§evrimiÃ§i eÄŸitim platformu iÃ§in slogan yaz. 6 ile 8 kelime arasÄ±nda olmalÄ±. Sadece sloganÄ± yaz.

**20. One-line advice (one sentence; include a time-management action)**  
- EN: Give one sentence of advice to a student preparing for exams. The sentence must include a concrete time-management action (e.g., plan, schedule, time-block). Output only the sentence.  
- DE: Gib einen Satz Ratschlag fÃ¼r einen Studenten, der sich auf PrÃ¼fungen vorbereitet. Der Satz muss eine konkrete Zeitmanagement-Aktion enthalten (z.B. planen, ZeitblÃ¶cke erstellen). Gib nur den Satz aus.  
- TR: SÄ±navlara hazÄ±rlanan bir Ã¶ÄŸrenciye tek cÃ¼mlelik tavsiye ver. CÃ¼mle somut bir zaman yÃ¶netimi eylemi iÃ§ermeli (Ã¶r. planla, programla, zaman bloklarÄ± oluÅŸtur). Sadece cÃ¼mleyi yaz.

---

## 11) Final Results Summary (Post-Implementation)

### Key Findings

| Finding | Value | Significance |
|---------|-------|--------------|
| Best overall model | **gemma3:4b** | 0.70 average cross-lingual similarity |
| Worst overall model | **phi3:latest** | 0.57 average (severe TR issues) |
| Most consistent task | **Factual** | 95.8% cross-lingual match rate |
| Least consistent task | **Reasoning** | 37.5% cross-lingual match rate |
| Hardest language pair | **EN-TR** | 0.586 average similarity |
| Easiest language pair | **DE-TR** | 0.670 average similarity |

### Model Rankings (Cross-Lingual Consistency)

| Rank | Model | Avg Similarity | Discrete Match |
|------|-------|----------------|----------------|
| 1 | gemma3:4b | 0.6969 | 91.7% |
| 2 | phi4-mini:3.8b | 0.6740 | 70.8% |
| 3 | gemma3:1b | 0.6613 | 75.0% |
| 4 | llama3.2:3b | 0.6172 | 50.0% |
| 5 | llama3.2:1b | 0.6046 | 54.2% |
| 6 | phi3:latest | 0.5696 | 45.8% |

### Recommendations

| Use Case | Recommended Model | Avoid |
|----------|-------------------|-------|
| Classification (all languages) | gemma3:4b, gemma3:1b | llama3.2:1b |
| Factual Q&A | Any model | - |
| Summarization | gemma3:4b, phi4-mini:3.8b | phi3:latest |
| Creative tasks | gemma3:4b (EN/DE only) | phi3:latest (TR) |
| Turkish language support | gemma3:4b | phi3:latest |

### Deliverables Produced

| Deliverable | Location | Status |
|-------------|----------|--------|
| Response archive | `data/responses.jsonl` | âœ… 720 records |
| Cross-lingual metrics | `data/metrics.csv` | âœ… 288 records |
| Stability metrics | `data/stability.csv` | âœ… 357 records |
| Discrete metrics | `data/task_metrics.csv` | âœ… 145 records |
| Visualizations | `outputs/plots/` | âœ… 31 files |
| Model summaries | `outputs/reports/` | âœ… 7 files |
| Final report | `REPORT_FINAL.md` | âœ… ~700 lines |

---

**End of Document**

**Document History:**
- v1.0 (17 Jan 2026): Initial PRD/TPRD
- v2.0 (17 Jan 2026): Post-implementation update with actual results and deviations
