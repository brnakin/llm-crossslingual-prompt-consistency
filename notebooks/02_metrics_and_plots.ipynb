{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Metrics and Plots\n",
        "\n",
        "This notebook computes all evaluation metrics and generates visualizations:\n",
        "- LaBSE embeddings and cross-lingual cosine similarity for open-text tasks\n",
        "- Task-aware agreement checks for discrete-answer tasks\n",
        "- Intra-language stability baselines (run1 vs run2)\n",
        "- Heatmaps, distribution plots, and summary tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add parent directory to path for imports\n",
        "import sys\n",
        "sys.path.insert(0, '..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.infer_ollama import load_responses, get_models\n",
        "from src.similarity import (\n",
        "    compute_all_metrics, \n",
        "    load_metrics, \n",
        "    load_stability,\n",
        "    aggregate_by_language_pair,\n",
        "    aggregate_by_task_type,\n",
        "    get_flagged_examples\n",
        ")\n",
        "from src.task_checks import (\n",
        "    compute_task_metrics,\n",
        "    load_task_metrics,\n",
        "    aggregate_task_metrics_by_task_type,\n",
        "    get_mismatched_examples\n",
        ")\n",
        "from src.plots import generate_all_plots\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total responses: 720\n",
            "\n",
            "By task type:\n",
            "task_type\n",
            "classification    144\n",
            "creative          144\n",
            "factual           144\n",
            "reasoning         144\n",
            "summarization     144\n",
            "dtype: int64\n",
            "\n",
            "Open-text responses: 288\n",
            "Discrete-answer responses: 432\n"
          ]
        }
      ],
      "source": [
        "# Load all responses\n",
        "responses = load_responses()\n",
        "responses_df = pd.DataFrame(responses)\n",
        "\n",
        "print(f\"Total responses: {len(responses_df)}\")\n",
        "print(f\"\\nBy task type:\")\n",
        "print(responses_df.groupby('task_type').size())\n",
        "\n",
        "# Separate open-text and discrete responses\n",
        "open_text_tasks = ['summarization', 'creative']\n",
        "discrete_tasks = ['classification', 'reasoning', 'factual']\n",
        "\n",
        "open_text_df = responses_df[responses_df['task_type'].isin(open_text_tasks)]\n",
        "discrete_df = responses_df[responses_df['task_type'].isin(discrete_tasks)]\n",
        "\n",
        "print(f\"\\nOpen-text responses: {len(open_text_df)}\")\n",
        "print(f\"Discrete-answer responses: {len(discrete_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Compute Open-Text Metrics (LaBSE Similarity)\n",
        "\n",
        "This section:\n",
        "1. Loads the LaBSE model\n",
        "2. Generates embeddings for all open-text responses\n",
        "3. Computes cross-lingual cosine similarity (EN-DE, EN-TR, DE-TR)\n",
        "4. Computes intra-language stability (run1 vs run2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 288 open-text responses...\n",
            "Loading LaBSE model: sentence-transformers/LaBSE\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "123f58dba8754ec4aedf1d636931ecff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ae3f6d87a954f1a865acc71a2a18d1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66e7c2047b5c426a87c373823ce401bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e1027dc25d34fafa190f941d7391d7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc84140265f14a9ea44b424c0d4fa113",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6a627ffb01b4900994fdb152e34897c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c949caf2f5a4fe7b9f492eede215b92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "934ebd3683664bd4b12a4b062749feae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea81a9eeaf8f49d2999434bb0c5463dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f90c044afec47a5989815b553111afa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6ae202246c8414a8fee36409b1057a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "904c757d14b446e488795543dafa4c39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a7720f5e1364805973f7e73a730554e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "2_Dense/model.safetensors:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing embeddings for 288 responses...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9aa91b3a1b44ee993bb5ffbb79c5d88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metrics to /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../data/metrics.csv\n",
            "Saved stability to /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../data/stability.csv\n",
            "\n",
            "Metrics computed: 288 cross-lingual comparisons\n",
            "Stability computed: 144 intra-language comparisons\n"
          ]
        }
      ],
      "source": [
        "# Compute open-text metrics\n",
        "# This will load LaBSE and compute embeddings (may take a few minutes on first run)\n",
        "\n",
        "metrics_df, stability_df = compute_all_metrics(show_progress=True)\n",
        "\n",
        "print(f\"\\nMetrics computed: {len(metrics_df)} cross-lingual comparisons\")\n",
        "print(f\"Stability computed: {len(stability_df)} intra-language comparisons\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-lingual similarity by language pair:\n",
            "\n",
            "gemma3:1b:\n",
            "      cosine_similarity              \n",
            "                   mean     std count\n",
            "pair                                 \n",
            "DE-TR            0.6982  0.1028    16\n",
            "EN-DE            0.6564  0.1030    16\n",
            "EN-TR            0.6293  0.1114    16\n",
            "\n",
            "gemma3:4b:\n",
            "      cosine_similarity              \n",
            "                   mean     std count\n",
            "pair                                 \n",
            "DE-TR            0.7336  0.0928    16\n",
            "EN-DE            0.7186  0.0972    16\n",
            "EN-TR            0.6385  0.1254    16\n",
            "\n",
            "llama3.2:1b:\n",
            "      cosine_similarity              \n",
            "                   mean     std count\n",
            "pair                                 \n",
            "DE-TR            0.6559  0.1707    16\n",
            "EN-DE            0.5833  0.1114    16\n",
            "EN-TR            0.5746  0.1445    16\n",
            "\n",
            "llama3.2:3b:\n",
            "      cosine_similarity              \n",
            "                   mean     std count\n",
            "pair                                 \n",
            "DE-TR            0.6525  0.0728    16\n",
            "EN-DE            0.5879  0.1033    16\n",
            "EN-TR            0.6110  0.1019    16\n",
            "\n",
            "phi3:latest:\n",
            "      cosine_similarity              \n",
            "                   mean     std count\n",
            "pair                                 \n",
            "DE-TR            0.5500  0.1483    16\n",
            "EN-DE            0.6937  0.0988    16\n",
            "EN-TR            0.4651  0.1539    16\n",
            "\n",
            "phi4-mini:3.8b:\n",
            "      cosine_similarity              \n",
            "                   mean     std count\n",
            "pair                                 \n",
            "DE-TR            0.7309  0.0915    16\n",
            "EN-DE            0.6806  0.0903    16\n",
            "EN-TR            0.6105  0.1247    16\n"
          ]
        }
      ],
      "source": [
        "# View metrics summary\n",
        "print(\"Cross-lingual similarity by language pair:\")\n",
        "for model_id in metrics_df['model_id'].unique():\n",
        "    print(f\"\\n{model_id}:\")\n",
        "    print(aggregate_by_language_pair(metrics_df, model_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-lingual similarity by task type:\n",
            "\n",
            "gemma3:1b:\n",
            "              cosine_similarity              \n",
            "                           mean     std count\n",
            "task_type                                    \n",
            "creative                 0.6077  0.1114    24\n",
            "summarization            0.7150  0.0720    24\n",
            "\n",
            "gemma3:4b:\n",
            "              cosine_similarity              \n",
            "                           mean     std count\n",
            "task_type                                    \n",
            "creative                 0.6550  0.1343    24\n",
            "summarization            0.7388  0.0629    24\n",
            "\n",
            "llama3.2:1b:\n",
            "              cosine_similarity              \n",
            "                           mean     std count\n",
            "task_type                                    \n",
            "creative                 0.4965  0.1002    24\n",
            "summarization            0.7128  0.0950    24\n",
            "\n",
            "llama3.2:3b:\n",
            "              cosine_similarity              \n",
            "                           mean     std count\n",
            "task_type                                    \n",
            "creative                 0.5815  0.0978    24\n",
            "summarization            0.6529  0.0803    24\n",
            "\n",
            "phi3:latest:\n",
            "              cosine_similarity              \n",
            "                           mean     std count\n",
            "task_type                                    \n",
            "creative                 0.4814  0.1539    24\n",
            "summarization            0.6578  0.1217    24\n",
            "\n",
            "phi4-mini:3.8b:\n",
            "              cosine_similarity              \n",
            "                           mean     std count\n",
            "task_type                                    \n",
            "creative                 0.6078  0.1010    24\n",
            "summarization            0.7402  0.0817    24\n"
          ]
        }
      ],
      "source": [
        "# View by task type\n",
        "print(\"Cross-lingual similarity by task type:\")\n",
        "for model_id in metrics_df['model_id'].unique():\n",
        "    print(f\"\\n{model_id}:\")\n",
        "    print(aggregate_by_task_type(metrics_df, model_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute Discrete-Answer Metrics (Task-Aware Checks)\n",
        "\n",
        "This section:\n",
        "1. Extracts labels/answers from discrete-answer responses\n",
        "2. Checks cross-lingual agreement (same run_id across languages)\n",
        "3. Checks intra-language stability (run1 vs run2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 432 discrete-answer responses...\n",
            "Saved task metrics to /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../data/task_metrics.csv\n",
            "Updated stability in /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../data/stability.csv\n",
            "\n",
            "Task metrics computed: 144 cross-lingual comparisons\n"
          ]
        }
      ],
      "source": [
        "# Compute task-aware metrics for discrete-answer tasks\n",
        "task_metrics_df, discrete_stability_df = compute_task_metrics(show_progress=True)\n",
        "\n",
        "print(f\"\\nTask metrics computed: {len(task_metrics_df)} cross-lingual comparisons\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-lingual agreement by task type:\n",
            "\n",
            "gemma3:4b:\n",
            "                total  matches  mismatches  uncertain  match_rate  \\\n",
            "task_type                                                           \n",
            "classification    8.0      8.0         0.0        0.0        1.00   \n",
            "factual           8.0      8.0         0.0        0.0        1.00   \n",
            "reasoning         8.0      6.0         2.0        0.0        0.75   \n",
            "\n",
            "                mismatch_rate  \n",
            "task_type                      \n",
            "classification           0.00  \n",
            "factual                  0.00  \n",
            "reasoning                0.25  \n",
            "\n",
            "gemma3:1b:\n",
            "                total  matches  mismatches  uncertain  match_rate  \\\n",
            "task_type                                                           \n",
            "classification    8.0      8.0         0.0        0.0        1.00   \n",
            "factual           8.0      8.0         0.0        0.0        1.00   \n",
            "reasoning         8.0      2.0         6.0        0.0        0.25   \n",
            "\n",
            "                mismatch_rate  \n",
            "task_type                      \n",
            "classification           0.00  \n",
            "factual                  0.00  \n",
            "reasoning                0.75  \n",
            "\n",
            "llama3.2:1b:\n",
            "                total  matches  mismatches  uncertain  match_rate  \\\n",
            "task_type                                                           \n",
            "classification    8.0      1.0         7.0        0.0       0.125   \n",
            "factual           8.0      8.0         0.0        0.0       1.000   \n",
            "reasoning         8.0      4.0         4.0        0.0       0.500   \n",
            "\n",
            "                mismatch_rate  \n",
            "task_type                      \n",
            "classification          0.875  \n",
            "factual                 0.000  \n",
            "reasoning               0.500  \n",
            "\n",
            "phi3:latest:\n",
            "                total  matches  mismatches  uncertain  match_rate  \\\n",
            "task_type                                                           \n",
            "classification    8.0      4.0         4.0        0.0       0.500   \n",
            "factual           8.0      6.0         2.0        0.0       0.750   \n",
            "reasoning         8.0      1.0         7.0        0.0       0.125   \n",
            "\n",
            "                mismatch_rate  \n",
            "task_type                      \n",
            "classification          0.500  \n",
            "factual                 0.250  \n",
            "reasoning               0.875  \n",
            "\n",
            "phi4-mini:3.8b:\n",
            "                total  matches  mismatches  uncertain  match_rate  \\\n",
            "task_type                                                           \n",
            "classification    8.0      6.0         2.0        0.0       0.750   \n",
            "factual           8.0      8.0         0.0        0.0       1.000   \n",
            "reasoning         8.0      3.0         5.0        0.0       0.375   \n",
            "\n",
            "                mismatch_rate  \n",
            "task_type                      \n",
            "classification          0.250  \n",
            "factual                 0.000  \n",
            "reasoning               0.625  \n",
            "\n",
            "llama3.2:3b:\n",
            "                total  matches  mismatches  uncertain  match_rate  \\\n",
            "task_type                                                           \n",
            "classification    8.0      2.0         6.0        0.0        0.25   \n",
            "factual           8.0      8.0         0.0        0.0        1.00   \n",
            "reasoning         8.0      2.0         6.0        0.0        0.25   \n",
            "\n",
            "                mismatch_rate  \n",
            "task_type                      \n",
            "classification           0.75  \n",
            "factual                  0.00  \n",
            "reasoning                0.75  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/task_checks.py:384: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby(\"task_type\").apply(compute_rates).round(4)\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/task_checks.py:384: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby(\"task_type\").apply(compute_rates).round(4)\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/task_checks.py:384: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby(\"task_type\").apply(compute_rates).round(4)\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/task_checks.py:384: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby(\"task_type\").apply(compute_rates).round(4)\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/task_checks.py:384: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby(\"task_type\").apply(compute_rates).round(4)\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/task_checks.py:384: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby(\"task_type\").apply(compute_rates).round(4)\n"
          ]
        }
      ],
      "source": [
        "# View task metrics summary\n",
        "print(\"Cross-lingual agreement by task type:\")\n",
        "for model_id in task_metrics_df['model_id'].unique():\n",
        "    print(f\"\\n{model_id}:\")\n",
        "    print(aggregate_task_metrics_by_task_type(task_metrics_df, model_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. View Stability Baseline\n",
        "\n",
        "Compare cross-lingual consistency against intra-language stability to separate language effects from model randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intra-language stability by type:\n",
            "                               mean       std  count\n",
            "stability_type   language                           \n",
            "discrete_match   DE        0.888889  0.316475     72\n",
            "                 EN        0.972222  0.165489     72\n",
            "                 TR        0.897059  0.306141     68\n",
            "open_text_cosine DE        0.853530  0.130855     48\n",
            "                 EN        0.863358  0.104380     48\n",
            "                 TR        0.827966  0.148075     48\n"
          ]
        }
      ],
      "source": [
        "# Load combined stability data\n",
        "stability_df = load_stability()\n",
        "\n",
        "print(\"Intra-language stability by type:\")\n",
        "print(stability_df.groupby(['stability_type', 'language'])['stability_value'].agg(['mean', 'std', 'count']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:135: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=df, x=\"pair\", y=\"cosine_similarity\", ax=ax1, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:252: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=stab_df, x=\"language\", y=\"stability_value\", ax=ax1, palette=\"Set3\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:264: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=met_df, x=\"pair\", y=\"cosine_similarity\", ax=ax2, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:135: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=df, x=\"pair\", y=\"cosine_similarity\", ax=ax1, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:252: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=stab_df, x=\"language\", y=\"stability_value\", ax=ax1, palette=\"Set3\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:264: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=met_df, x=\"pair\", y=\"cosine_similarity\", ax=ax2, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:135: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=df, x=\"pair\", y=\"cosine_similarity\", ax=ax1, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:252: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=stab_df, x=\"language\", y=\"stability_value\", ax=ax1, palette=\"Set3\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:264: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=met_df, x=\"pair\", y=\"cosine_similarity\", ax=ax2, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:135: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=df, x=\"pair\", y=\"cosine_similarity\", ax=ax1, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:252: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=stab_df, x=\"language\", y=\"stability_value\", ax=ax1, palette=\"Set3\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:264: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=met_df, x=\"pair\", y=\"cosine_similarity\", ax=ax2, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:135: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=df, x=\"pair\", y=\"cosine_similarity\", ax=ax1, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:252: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=stab_df, x=\"language\", y=\"stability_value\", ax=ax1, palette=\"Set3\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:264: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=met_df, x=\"pair\", y=\"cosine_similarity\", ax=ax2, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:135: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=df, x=\"pair\", y=\"cosine_similarity\", ax=ax1, palette=\"Set2\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:252: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=stab_df, x=\"language\", y=\"stability_value\", ax=ax1, palette=\"Set3\")\n",
            "/home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../src/plots.py:264: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(data=met_df, x=\"pair\", y=\"cosine_similarity\", ax=ax2, palette=\"Set2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated plots:\n",
            "\n",
            "heatmaps:\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/heatmap_phi3_latest.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/heatmap_phi4-mini_3.8b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/heatmap_gemma3_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/heatmap_gemma3_4b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/heatmap_llama3.2_3b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/heatmap_llama3.2_1b.png\n",
            "\n",
            "distributions:\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/distribution_phi3_latest.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/distribution_phi4-mini_3.8b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/distribution_gemma3_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/distribution_gemma3_4b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/distribution_llama3.2_3b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/distribution_llama3.2_1b.png\n",
            "\n",
            "comparisons:\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/task_comparison_phi3_latest.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/stability_phi3_latest.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/discrete_summary_phi3_latest.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/task_comparison_phi4-mini_3.8b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/stability_phi4-mini_3.8b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/discrete_summary_phi4-mini_3.8b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/task_comparison_gemma3_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/stability_gemma3_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/discrete_summary_gemma3_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/task_comparison_gemma3_4b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/stability_gemma3_4b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/discrete_summary_gemma3_4b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/task_comparison_llama3.2_3b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/stability_llama3.2_3b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/discrete_summary_llama3.2_3b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/task_comparison_llama3.2_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/stability_llama3.2_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/discrete_summary_llama3.2_1b.png\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/plots/model_comparison.png\n",
            "\n",
            "reports:\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/reports/summary_phi3_latest.csv\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/reports/summary_phi4-mini_3.8b.csv\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/reports/summary_gemma3_1b.csv\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/reports/summary_gemma3_4b.csv\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/reports/summary_llama3.2_3b.csv\n",
            "  - /home/baran/project-courses/llm-crossslingual-prompt-consistency/notebooks/../outputs/reports/summary_llama3.2_1b.csv\n"
          ]
        }
      ],
      "source": [
        "# Generate all plots and save them\n",
        "saved_paths = generate_all_plots(\n",
        "    metrics_df=metrics_df,\n",
        "    task_metrics_df=task_metrics_df,\n",
        "    stability_df=stability_df,\n",
        "    show=False  # Set to True to display inline\n",
        ")\n",
        "\n",
        "print(\"Generated plots:\")\n",
        "for plot_type, paths in saved_paths.items():\n",
        "    print(f\"\\n{plot_type}:\")\n",
        "    for path in paths:\n",
        "        print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. View Flagged Low-Similarity Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flagged low-similarity examples: 30\n",
            "\n",
            "Lowest similarity cases:\n",
            "        model_id  prompt_id task_type   pair  cosine_similarity\n",
            "232  phi3:latest         19  creative  EN-TR           0.240370\n",
            "139  llama3.2:1b         20  creative  EN-TR           0.285214\n",
            "226  phi3:latest         18  creative  EN-TR           0.297483\n",
            "229  phi3:latest         19  creative  EN-TR           0.310010\n",
            "136  llama3.2:1b         19  creative  EN-TR           0.323369\n",
            "223  phi3:latest         18  creative  EN-TR           0.332236\n",
            "230  phi3:latest         19  creative  DE-TR           0.343559\n",
            "40     gemma3:1b         19  creative  EN-TR           0.346787\n",
            "183  llama3.2:3b         19  creative  EN-DE           0.361225\n",
            "129  llama3.2:1b         18  creative  EN-DE           0.367049\n"
          ]
        }
      ],
      "source": [
        "# Get flagged examples (bottom 10% similarity)\n",
        "flagged = get_flagged_examples(metrics_df, responses)\n",
        "print(f\"Flagged low-similarity examples: {len(flagged)}\")\n",
        "\n",
        "if len(flagged) > 0:\n",
        "    print(\"\\nLowest similarity cases:\")\n",
        "    print(flagged[['model_id', 'prompt_id', 'task_type', 'pair', 'cosine_similarity']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mismatched discrete-answer examples: 51\n",
            "\n",
            "Mismatched cases:\n",
            "       model_id  prompt_id       task_type    key_en    key_de    key_tr\n",
            "12    gemma3:4b         11       reasoning         1        78         1\n",
            "13    gemma3:4b         11       reasoning         1        78         1\n",
            "34    gemma3:1b         10       reasoning      22.5        60        18\n",
            "35    gemma3:1b         10       reasoning      22.5        60        90\n",
            "36    gemma3:1b         11       reasoning        75         1        45\n",
            "37    gemma3:1b         11       reasoning        75        60         1\n",
            "38    gemma3:1b         12       reasoning         A         B         B\n",
            "39    gemma3:1b         12       reasoning         A         B         B\n",
            "48  llama3.2:1b          5  classification  negative  positive  positive\n",
            "49  llama3.2:1b          5  classification  negative  positive  positive\n"
          ]
        }
      ],
      "source": [
        "# Get mismatched discrete-answer examples\n",
        "mismatched = get_mismatched_examples(task_metrics_df, responses)\n",
        "print(f\"Mismatched discrete-answer examples: {len(mismatched)}\")\n",
        "\n",
        "if len(mismatched) > 0:\n",
        "    print(\"\\nMismatched cases:\")\n",
        "    print(mismatched[['model_id', 'prompt_id', 'task_type', 'key_en', 'key_de', 'key_tr']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All metrics have been computed and saved:\n",
        "- `data/metrics.csv` - Cross-lingual cosine similarities for open-text tasks\n",
        "- `data/stability.csv` - Intra-language stability (run1 vs run2)\n",
        "- `data/task_metrics.csv` - Discrete-answer agreement results\n",
        "- `outputs/plots/` - Heatmaps and distribution plots\n",
        "- `outputs/reports/` - Summary tables\n",
        "\n",
        "Proceed to `03_qualitative_review.ipynb` to examine flagged low-consistency examples in detail."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-crossslingual-prompt-consistency",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
