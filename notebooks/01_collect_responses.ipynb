{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Collect Responses\n",
        "\n",
        "This notebook runs inference for all prompts, languages, and models using Ollama.\n",
        "\n",
        "## Prerequisites\n",
        "- Ollama must be installed and running locally\n",
        "- Required models must be pulled: `ollama pull gemma3:1b` and `ollama pull llama3.2:1b`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add parent directory to path for imports\n",
        "import sys\n",
        "sys.path.insert(0, '..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.load_prompts import load_prompts, get_task_types, get_languages\n",
        "from src.infer_ollama import (\n",
        "    run_inference, \n",
        "    run_full_inference,\n",
        "    load_responses, \n",
        "    get_models, \n",
        "    get_inference_params,\n",
        ")\n",
        "from pathlib import Path\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Verify Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured Models:\n",
            "  - gemma3:1b: Gemma 3 1B\n",
            "  - llama3.2:1b: Llama 3.2 1B\n",
            "\n",
            "Inference Parameters:\n",
            "  temperature: 0.3\n",
            "  num_predict: 256\n",
            "  runs_per_prompt: 2\n"
          ]
        }
      ],
      "source": [
        "# Check configured models\n",
        "print(\"Configured Models:\")\n",
        "for model in get_models():\n",
        "    print(f\"  - {model['id']}: {model['name']}\")\n",
        "\n",
        "print(\"\\nInference Parameters:\")\n",
        "params = get_inference_params()\n",
        "for k, v in params.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify Prompts Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total prompts: 60\n",
            "\n",
            "Task types: ['classification', 'creative', 'factual', 'reasoning', 'summarization']\n",
            "Languages: ['DE', 'EN', 'TR']\n",
            "\n",
            "Prompts per task type:\n",
            "task_type\n",
            "classification    12\n",
            "creative          12\n",
            "factual           12\n",
            "reasoning         12\n",
            "summarization     12\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load and inspect prompts\n",
        "prompts_df = load_prompts(prepend_control_line=False)\n",
        "print(f\"Total prompts: {len(prompts_df)}\")\n",
        "print(f\"\\nTask types: {get_task_types()}\")\n",
        "print(f\"Languages: {get_languages()}\")\n",
        "print(f\"\\nPrompts per task type:\")\n",
        "print(prompts_df.groupby('task_type').size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Check Ollama Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Ollama models:\n",
            "Error connecting to Ollama: 'name'\n",
            "\n",
            "Make sure Ollama is running: `ollama serve`\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "try:\n",
        "    models_list = ollama.list()\n",
        "    print(\"Available Ollama models:\")\n",
        "    for model in models_list.get('models', []):\n",
        "        print(f\"  - {model['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Ollama: {e}\")\n",
        "    print(\"\\nMake sure Ollama is running: `ollama serve`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Inference\n",
        "\n",
        "This will generate responses for all prompts, languages, runs, and models.\n",
        "\n",
        "**Expected calls:** 20 prompts × 3 languages × 2 runs × 2 models = **240 calls**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Existing responses: 360\n",
            "\n",
            "By model:\n",
            "model_id\n",
            "gemma3:1b      120\n",
            "gemma3:4b      120\n",
            "llama3.2:1b    120\n",
            "dtype: int64\n",
            "\n",
            "By language:\n",
            "language\n",
            "DE    120\n",
            "EN    120\n",
            "TR    120\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check existing responses\n",
        "existing = load_responses()\n",
        "print(f\"Existing responses: {len(existing)}\")\n",
        "\n",
        "if len(existing) > 0:\n",
        "    existing_df = pd.DataFrame(existing)\n",
        "    print(\"\\nBy model:\")\n",
        "    print(existing_df.groupby('model_id').size())\n",
        "    print(\"\\nBy language:\")\n",
        "    print(existing_df.groupby('language').size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total inference calls needed: 360 (skipped 360 existing)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating responses: 100%|██████████| 360/360 [07:35<00:00,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated 360 new responses\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run full inference (skips existing by default)\n",
        "# This may take 10-30 minutes depending on hardware\n",
        "\n",
        "count = run_full_inference(skip_existing=True)\n",
        "print(f\"\\nGenerated {count} new responses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verify Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total responses: 720\n",
            "\n",
            "Expected: 720 (20 prompts × 3 langs × 2 runs × 6 models)\n",
            "\n",
            "Coverage by model and language:\n",
            "language        DE  EN  TR\n",
            "model_id                  \n",
            "gemma3:1b       40  40  40\n",
            "gemma3:4b       40  40  40\n",
            "llama3.2:1b     40  40  40\n",
            "llama3.2:3b     40  40  40\n",
            "phi3:latest     40  40  40\n",
            "phi4-mini:3.8b  40  40  40\n"
          ]
        }
      ],
      "source": [
        "# Load and verify all responses\n",
        "responses = load_responses()\n",
        "responses_df = pd.DataFrame(responses)\n",
        "\n",
        "print(f\"Total responses: {len(responses_df)}\")\n",
        "print(f\"\\nExpected: {20 * 3 * 2 * len(get_models())} (20 prompts × 3 langs × 2 runs × {len(get_models())} models)\")\n",
        "\n",
        "# Check coverage\n",
        "print(\"\\nCoverage by model and language:\")\n",
        "coverage = responses_df.groupby(['model_id', 'language']).size().unstack(fill_value=0)\n",
        "print(coverage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Non-compliant responses: 53\n",
            "\n",
            "Non-compliant by task type:\n",
            "task_type\n",
            "classification    32\n",
            "factual           11\n",
            "reasoning         10\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for non-compliant responses\n",
        "non_compliant = responses_df[responses_df['non_compliant'] == True]\n",
        "print(f\"Non-compliant responses: {len(non_compliant)}\")\n",
        "\n",
        "if len(non_compliant) > 0:\n",
        "    print(\"\\nNon-compliant by task type:\")\n",
        "    print(non_compliant.groupby('task_type').size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Retry Non-Compliant Responses\n",
        "\n",
        "Re-generate responses that violated format constraints with a stricter prompt wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrying 1 non-compliant responses...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying: 100%|██████████| 1/1 [00:09<00:00,  9.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retried 1 responses.\n",
            "Still non-compliant after retry: 1\n",
            "\n",
            "Final non-compliant count: 1\n",
            "Remaining non-compliant by task type:\n",
            "task_type\n",
            "classification    1\n",
            "dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import ollama\n",
        "\n",
        "def get_stricter_control_line(language: str) -> str:\n",
        "    \"\"\"Get stricter control line for retry attempts.\"\"\"\n",
        "    lines = {\n",
        "        \"EN\": \"IMPORTANT: Output ONLY the answer. No explanations, no reasoning, no extra text. Just the answer.\",\n",
        "        \"DE\": \"WICHTIG: Gib NUR die Antwort aus. Keine Erklärungen, keine Begründung, kein zusätzlicher Text. Nur die Antwort.\",\n",
        "        \"TR\": \"ÖNEMLİ: SADECE cevabı yaz. Açıklama yok, gerekçe yok, fazladan metin yok. Sadece cevap.\"\n",
        "    }\n",
        "    return lines.get(language, lines[\"EN\"])\n",
        "\n",
        "def retry_non_compliant_responses(responses_df, responses_file: Path):\n",
        "    \"\"\"Retry non-compliant responses with stricter prompts and overwrite.\"\"\"\n",
        "    \n",
        "    non_compliant = responses_df[responses_df['non_compliant'] == True]\n",
        "    \n",
        "    if len(non_compliant) == 0:\n",
        "        print(\"No non-compliant responses to retry.\")\n",
        "        return 0\n",
        "    \n",
        "    print(f\"Retrying {len(non_compliant)} non-compliant responses...\")\n",
        "    \n",
        "    # Load all responses\n",
        "    all_responses = []\n",
        "    with open(responses_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                all_responses.append(json.loads(line))\n",
        "    \n",
        "    # Create index for quick lookup\n",
        "    response_index = {}\n",
        "    for i, r in enumerate(all_responses):\n",
        "        key = (r['model_id'], r['prompt_id'], r['language'], r['run_id'])\n",
        "        response_index[key] = i\n",
        "    \n",
        "    # Retry each non-compliant response\n",
        "    retried_count = 0\n",
        "    for _, row in tqdm(non_compliant.iterrows(), total=len(non_compliant), desc=\"Retrying\"):\n",
        "        key = (row['model_id'], row['prompt_id'], row['language'], row['run_id'])\n",
        "        \n",
        "        if key not in response_index:\n",
        "            continue\n",
        "        \n",
        "        idx = response_index[key]\n",
        "        original = all_responses[idx]\n",
        "        \n",
        "        # Create stricter prompt\n",
        "        stricter_control = get_stricter_control_line(row['language'])\n",
        "        original_prompt = original['prompt_text']\n",
        "        stricter_prompt = f\"{stricter_control}\\n\\n{original_prompt}\"\n",
        "        \n",
        "        try:\n",
        "            # Generate new response\n",
        "            response = ollama.generate(\n",
        "                model=row['model_id'],\n",
        "                prompt=stricter_prompt,\n",
        "                options={\n",
        "                    \"temperature\": original['temperature'],\n",
        "                    \"num_predict\": original['max_new_tokens']\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            new_response_text = response[\"response\"]\n",
        "            \n",
        "            # Check if new response is compliant\n",
        "            from src.infer_ollama import detect_non_compliance\n",
        "            new_non_compliant = detect_non_compliance(\n",
        "                new_response_text, \n",
        "                row['task_type'], \n",
        "                row['prompt_id']\n",
        "            )\n",
        "            \n",
        "            # Update the response\n",
        "            all_responses[idx]['response_text'] = new_response_text\n",
        "            all_responses[idx]['non_compliant'] = new_non_compliant\n",
        "            all_responses[idx]['timestamp_utc'] = datetime.now(timezone.utc).isoformat()\n",
        "            all_responses[idx]['retry'] = True\n",
        "            \n",
        "            retried_count += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error retrying {key}: {e}\")\n",
        "    \n",
        "    # Write back all responses\n",
        "    with open(responses_file, 'w', encoding='utf-8') as f:\n",
        "        for r in all_responses:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
        "    \n",
        "    print(f\"\\nRetried {retried_count} responses.\")\n",
        "    \n",
        "    # Check how many are still non-compliant\n",
        "    still_non_compliant = sum(1 for r in all_responses if r.get('non_compliant', False))\n",
        "    print(f\"Still non-compliant after retry: {still_non_compliant}\")\n",
        "    \n",
        "    return retried_count\n",
        "\n",
        "# Run retry\n",
        "responses_file = Path('../data/responses.jsonl')\n",
        "retry_non_compliant_responses(responses_df, responses_file)\n",
        "\n",
        "# Reload and show updated stats\n",
        "responses = load_responses()\n",
        "responses_df = pd.DataFrame(responses)\n",
        "non_compliant_after = responses_df[responses_df['non_compliant'] == True]\n",
        "print(f\"\\nFinal non-compliant count: {len(non_compliant_after)}\")\n",
        "if len(non_compliant_after) > 0:\n",
        "    print(\"Remaining non-compliant by task type:\")\n",
        "    print(non_compliant_after.groupby('task_type').size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Proceed to `02_metrics_and_plots.ipynb` to:\n",
        "1. Compute LaBSE embeddings for open-text responses\n",
        "2. Calculate cross-lingual similarity metrics\n",
        "3. Run task-aware checks for discrete-answer tasks (non-compliant responses will be handled automatically)\n",
        "4. Generate visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Re-evaluate All Responses with Updated Rules\n",
        "\n",
        "Re-check all responses with the updated (more lenient) non-compliance detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated 0 response statuses\n",
            "\n",
            "Non-compliant responses after re-evaluation: 1\n",
            "\n",
            "Non-compliant by task type:\n",
            "task_type\n",
            "classification    1\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Reload the module to get updated detect_non_compliance function\n",
        "import importlib\n",
        "import src.infer_ollama\n",
        "importlib.reload(src.infer_ollama)\n",
        "from src.infer_ollama import detect_non_compliance, load_responses\n",
        "\n",
        "# Re-evaluate all responses\n",
        "responses_file = Path('../data/responses.jsonl')\n",
        "\n",
        "all_responses = []\n",
        "with open(responses_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            all_responses.append(json.loads(line))\n",
        "\n",
        "# Re-check non-compliance with updated rules\n",
        "updated_count = 0\n",
        "for r in all_responses:\n",
        "    old_status = r.get('non_compliant', False)\n",
        "    new_status = detect_non_compliance(r['response_text'], r['task_type'], r['prompt_id'])\n",
        "    \n",
        "    if old_status != new_status:\n",
        "        r['non_compliant'] = new_status\n",
        "        updated_count += 1\n",
        "\n",
        "# Save updated responses\n",
        "with open(responses_file, 'w', encoding='utf-8') as f:\n",
        "    for r in all_responses:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"Updated {updated_count} response statuses\")\n",
        "\n",
        "# Show new stats\n",
        "non_compliant_count = sum(1 for r in all_responses if r.get('non_compliant', False))\n",
        "print(f\"\\nNon-compliant responses after re-evaluation: {non_compliant_count}\")\n",
        "\n",
        "# Show by task type\n",
        "responses_df = pd.DataFrame(all_responses)\n",
        "non_compliant_df = responses_df[responses_df['non_compliant'] == True]\n",
        "if len(non_compliant_df) > 0:\n",
        "    print(\"\\nNon-compliant by task type:\")\n",
        "    print(non_compliant_df.groupby('task_type').size())\n",
        "else:\n",
        "    print(\"\\nAll responses are now compliant!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-crossslingual-prompt-consistency",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
