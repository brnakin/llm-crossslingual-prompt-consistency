{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Qualitative Review\n",
        "\n",
        "This notebook provides tools for reviewing flagged low-consistency examples:\n",
        "- Display prompts and responses side-by-side across languages\n",
        "- Annotate drift types (semantic, format, factual)\n",
        "- Generate a qualitative evidence set for the report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add parent directory to path for imports\n",
        "import sys\n",
        "sys.path.insert(0, '..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.infer_ollama import load_responses, get_responses_by_prompt\n",
        "from src.load_prompts import load_prompts\n",
        "from src.similarity import load_metrics, get_flagged_examples\n",
        "from src.task_checks import load_task_metrics, get_mismatched_examples\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 720 responses\n",
            "Loaded 60 prompts\n",
            "Loaded 288 metric records\n",
            "Loaded 144 task metric records\n"
          ]
        }
      ],
      "source": [
        "# Load all data\n",
        "responses = load_responses()\n",
        "prompts_df = load_prompts(prepend_control_line=False)\n",
        "metrics_df = load_metrics()\n",
        "task_metrics_df = load_task_metrics()\n",
        "\n",
        "print(f\"Loaded {len(responses)} responses\")\n",
        "print(f\"Loaded {len(prompts_df)} prompts\")\n",
        "print(f\"Loaded {len(metrics_df)} metric records\")\n",
        "print(f\"Loaded {len(task_metrics_df)} task metric records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_prompt(prompt_id: int):\n",
        "    \"\"\"Display a prompt in all three languages.\"\"\"\n",
        "    prompt_rows = prompts_df[prompts_df['prompt_id'] == prompt_id]\n",
        "    \n",
        "    if len(prompt_rows) == 0:\n",
        "        print(f\"Prompt {prompt_id} not found\")\n",
        "        return\n",
        "    \n",
        "    task_type = prompt_rows.iloc[0]['task_type']\n",
        "    print(f\"=== Prompt {prompt_id} ({task_type}) ===\\n\")\n",
        "    \n",
        "    for lang in ['EN', 'DE', 'TR']:\n",
        "        row = prompt_rows[prompt_rows['language'] == lang]\n",
        "        if len(row) > 0:\n",
        "            print(f\"[{lang}]\")\n",
        "            print(row.iloc[0]['text'])\n",
        "            print()\n",
        "\n",
        "\n",
        "def display_responses(prompt_id: int, model_id: str, run_id: int = 1):\n",
        "    \"\"\"Display responses for a prompt across all languages.\"\"\"\n",
        "    resp_dict = {}\n",
        "    for r in responses:\n",
        "        if r['prompt_id'] == prompt_id and r['model_id'] == model_id and r['run_id'] == run_id:\n",
        "            resp_dict[r['language']] = r['response_text']\n",
        "    \n",
        "    print(f\"=== Responses (Model: {model_id}, Run: {run_id}) ===\\n\")\n",
        "    \n",
        "    for lang in ['EN', 'DE', 'TR']:\n",
        "        if lang in resp_dict:\n",
        "            print(f\"[{lang}]\")\n",
        "            print(resp_dict[lang])\n",
        "            print()\n",
        "\n",
        "\n",
        "def display_full_comparison(prompt_id: int, model_id: str, run_id: int = 1):\n",
        "    \"\"\"Display prompt and responses side by side.\"\"\"\n",
        "    display_prompt(prompt_id)\n",
        "    print(\"-\" * 80)\n",
        "    display_responses(prompt_id, model_id, run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Review Flagged Open-Text Examples\n",
        "\n",
        "These are the bottom 10% similarity cases for open-text tasks (summarization, creative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total flagged examples: 30\n",
            "\n",
            "Flagged cases sorted by similarity:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>task_type</th>\n",
              "      <th>pair</th>\n",
              "      <th>run_id</th>\n",
              "      <th>cosine_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>phi3:latest</td>\n",
              "      <td>19</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>2</td>\n",
              "      <td>0.240370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>llama3.2:1b</td>\n",
              "      <td>20</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>1</td>\n",
              "      <td>0.285214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>phi3:latest</td>\n",
              "      <td>18</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>2</td>\n",
              "      <td>0.297483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>phi3:latest</td>\n",
              "      <td>19</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>1</td>\n",
              "      <td>0.310010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>llama3.2:1b</td>\n",
              "      <td>19</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>2</td>\n",
              "      <td>0.323369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>phi3:latest</td>\n",
              "      <td>18</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>1</td>\n",
              "      <td>0.332236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>phi3:latest</td>\n",
              "      <td>19</td>\n",
              "      <td>creative</td>\n",
              "      <td>DE-TR</td>\n",
              "      <td>1</td>\n",
              "      <td>0.343559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>19</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>2</td>\n",
              "      <td>0.346787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>llama3.2:3b</td>\n",
              "      <td>19</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-DE</td>\n",
              "      <td>2</td>\n",
              "      <td>0.361225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>llama3.2:1b</td>\n",
              "      <td>18</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-DE</td>\n",
              "      <td>2</td>\n",
              "      <td>0.367049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        model_id  prompt_id task_type   pair  run_id  cosine_similarity\n",
              "232  phi3:latest         19  creative  EN-TR       2           0.240370\n",
              "139  llama3.2:1b         20  creative  EN-TR       1           0.285214\n",
              "226  phi3:latest         18  creative  EN-TR       2           0.297483\n",
              "229  phi3:latest         19  creative  EN-TR       1           0.310010\n",
              "136  llama3.2:1b         19  creative  EN-TR       2           0.323369\n",
              "223  phi3:latest         18  creative  EN-TR       1           0.332236\n",
              "230  phi3:latest         19  creative  DE-TR       1           0.343559\n",
              "40     gemma3:1b         19  creative  EN-TR       2           0.346787\n",
              "183  llama3.2:3b         19  creative  EN-DE       2           0.361225\n",
              "129  llama3.2:1b         18  creative  EN-DE       2           0.367049"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get flagged examples\n",
        "flagged = get_flagged_examples(metrics_df, responses)\n",
        "print(f\"Total flagged examples: {len(flagged)}\")\n",
        "\n",
        "if len(flagged) > 0:\n",
        "    print(\"\\nFlagged cases sorted by similarity:\")\n",
        "    display(flagged[['model_id', 'prompt_id', 'task_type', 'pair', 'run_id', 'cosine_similarity']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviewing lowest similarity case:\n",
            "  Similarity: 0.2404\n",
            "  Pair: EN-TR\n",
            "\n",
            "=== Prompt 19 (creative) ===\n",
            "\n",
            "[EN]\n",
            "Write a slogan for an online education platform. It must be 6 to 8 words. Output only the slogan.\n",
            "\n",
            "[DE]\n",
            "Schreibe einen Slogan für eine Online-Bildungsplattform. Er muss aus 6 bis 8 Wörtern bestehen. Gib nur den Slogan aus.\n",
            "\n",
            "[TR]\n",
            "Bir çevrimiçi eğitim platformu için slogan yaz. 6 ile 8 kelime arasında olmalı. Sadece sloganı yaz.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "=== Responses (Model: phi3:latest, Run: 2) ===\n",
            "\n",
            "[EN]\n",
            "\"Unlock Potential, Anywhere.\"\n",
            "\n",
            "[DE]\n",
            "\"Lernen, Fortschritt, Weltweiter Zugang!\"\n",
            "\n",
            "[TR]\n",
            "\"Çalışma, Öncelikler ve Destek Yapmak!\" - Bir çevrimiçi eğitim platformu için bir slogan oluyoruz. Bu kelimenin arasında \"Çalışma\", özellikle başlangıc atılması ve gelişmeyi belirtir; \"Öncelikler\", önceden eğitim altyapısı, yazan bir süreçte olan konuya geri kalma; \"Destek Yapmak\" de özgü bağlamlarda ve geliştirmeyi belirtir.\n",
            "\n",
            "Çünkü 6-8 kelime arasında olmalıdır, sloganu daha doğalgazlı bir anlayışla ilerlemektedir: \"Bahsedimiz ve Geliştirmemizi Sağlar!\"\n",
            "\n",
            "Çünkü 6-8 kelime arasında olmalıdır, slog\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Review the lowest similarity case\n",
        "if len(flagged) > 0:\n",
        "    worst = flagged.iloc[0]\n",
        "    print(f\"Reviewing lowest similarity case:\")\n",
        "    print(f\"  Similarity: {worst['cosine_similarity']:.4f}\")\n",
        "    print(f\"  Pair: {worst['pair']}\")\n",
        "    print()\n",
        "    display_full_comparison(worst['prompt_id'], worst['model_id'], worst['run_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Review Mismatched Discrete-Answer Examples\n",
        "\n",
        "These are cases where the extracted answer differs across languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total mismatched examples: 51\n",
            "\n",
            "Mismatched cases:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>task_type</th>\n",
              "      <th>run_id</th>\n",
              "      <th>key_en</th>\n",
              "      <th>key_de</th>\n",
              "      <th>key_tr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>gemma3:4b</td>\n",
              "      <td>11</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>gemma3:4b</td>\n",
              "      <td>11</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>10</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>1</td>\n",
              "      <td>22.5</td>\n",
              "      <td>60</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>10</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>2</td>\n",
              "      <td>22.5</td>\n",
              "      <td>60</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>11</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>1</td>\n",
              "      <td>75</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>11</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>2</td>\n",
              "      <td>75</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>12</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>12</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>llama3.2:1b</td>\n",
              "      <td>5</td>\n",
              "      <td>classification</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>llama3.2:1b</td>\n",
              "      <td>5</td>\n",
              "      <td>classification</td>\n",
              "      <td>2</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       model_id  prompt_id       task_type  run_id    key_en    key_de  \\\n",
              "12    gemma3:4b         11       reasoning       1         1        78   \n",
              "13    gemma3:4b         11       reasoning       2         1        78   \n",
              "34    gemma3:1b         10       reasoning       1      22.5        60   \n",
              "35    gemma3:1b         10       reasoning       2      22.5        60   \n",
              "36    gemma3:1b         11       reasoning       1        75         1   \n",
              "37    gemma3:1b         11       reasoning       2        75        60   \n",
              "38    gemma3:1b         12       reasoning       1         A         B   \n",
              "39    gemma3:1b         12       reasoning       2         A         B   \n",
              "48  llama3.2:1b          5  classification       1  negative  positive   \n",
              "49  llama3.2:1b          5  classification       2  negative  positive   \n",
              "\n",
              "      key_tr  \n",
              "12         1  \n",
              "13         1  \n",
              "34        18  \n",
              "35        90  \n",
              "36        45  \n",
              "37         1  \n",
              "38         B  \n",
              "39         B  \n",
              "48  positive  \n",
              "49  positive  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get mismatched examples\n",
        "mismatched = get_mismatched_examples(task_metrics_df, responses)\n",
        "print(f\"Total mismatched examples: {len(mismatched)}\")\n",
        "\n",
        "if len(mismatched) > 0:\n",
        "    print(\"\\nMismatched cases:\")\n",
        "    display(mismatched[['model_id', 'prompt_id', 'task_type', 'run_id', 'key_en', 'key_de', 'key_tr']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviewing mismatched case:\n",
            "  Task: reasoning\n",
            "  Extracted: EN=1, DE=78, TR=1\n",
            "\n",
            "=== Prompt 11 (reasoning) ===\n",
            "\n",
            "[EN]\n",
            "A meeting started at 9:15 and ended at 10:45. How many minutes did it last? Output only the number of minutes.\n",
            "\n",
            "[DE]\n",
            "Ein Meeting begann um 9:15 und endete um 10:45. Wie viele Minuten dauerte es? Gib nur die Minutenanzahl aus.\n",
            "\n",
            "[TR]\n",
            "Toplantı 9:15'te başlayıp 10:45'te bitti. Kaç dakika sürdü? Sadece dakika sayısını yaz.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "=== Responses (Model: gemma3:4b, Run: 1) ===\n",
            "\n",
            "[EN]\n",
            "1 hour and 30 minutes\n",
            "90 minutes\n",
            "\n",
            "[DE]\n",
            "78\n",
            "\n",
            "\n",
            "[TR]\n",
            "1 saat 30 dakika\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Review a mismatched case\n",
        "if len(mismatched) > 0:\n",
        "    case = mismatched.iloc[0]\n",
        "    print(f\"Reviewing mismatched case:\")\n",
        "    print(f\"  Task: {case['task_type']}\")\n",
        "    print(f\"  Extracted: EN={case['key_en']}, DE={case['key_de']}, TR={case['key_tr']}\")\n",
        "    print()\n",
        "    display_full_comparison(case['prompt_id'], case['model_id'], case['run_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Interactive Review\n",
        "\n",
        "Use this cell to review any specific prompt/model/run combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Prompt 1 (summarization) ===\n",
            "\n",
            "[EN]\n",
            "Summarize the following paragraph in one sentence: \"Artificial intelligence is changing many industries by automating repetitive work and helping people make faster decisions. Companies use AI to detect patterns in large datasets, but the results depend on data quality and careful evaluation. While productivity can increase, some tasks may be replaced and employees may need reskilling. Clear policies are also needed to reduce privacy risks and unfair outcomes.\"\n",
            "\n",
            "[DE]\n",
            "Fasse den folgenden Absatz in einem Satz zusammen: „Künstliche Intelligenz verändert viele Branchen, indem sie repetitive Arbeit automatisiert und Menschen hilft, schneller Entscheidungen zu treffen. Unternehmen nutzen KI, um Muster in großen Datensätzen zu erkennen, aber die Ergebnisse hängen von Datenqualität und sorgfältiger Bewertung ab. Obwohl die Produktivität steigen kann, können einige Tätigkeiten ersetzt werden und Beschäftigte benötigen möglicherweise Umschulungen. Außerdem sind klare Richtlinien nötig, um Datenschutzrisiken und unfairen Ergebnissen vorzubeugen.\"\n",
            "\n",
            "[TR]\n",
            "Aşağıdaki paragrafı tek cümlede özetle: \"Yapay zeka, tekrarlayan işleri otomatikleştirerek ve daha hızlı karar vermeye yardımcı olarak birçok sektörü dönüştürüyor. Şirketler büyük veri kümelerinde örüntüleri yakalamak için yapay zekayı kullanıyor, ancak sonuçlar veri kalitesine ve dikkatli değerlendirmeye bağlıdır. Verimlilik artabilse de bazı görevler ortadan kalkabilir ve çalışanların yeniden beceri kazanması gerekebilir. Ayrıca gizlilik risklerini ve adaletsiz sonuçları azaltmak için net politikalar gerekir.\"\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "=== Responses (Model: gemma3:1b, Run: 1) ===\n",
            "\n",
            "[EN]\n",
            "Artificial intelligence is transforming industries through automation and data-driven decision-making, requiring careful consideration of data quality, ethical implications, and workforce adaptation.\n",
            "\n",
            "[DE]\n",
            "Künstliche Intelligenz verändert Branchen durch Automatisierung und beschleunigte Entscheidungsfindung, erfordert aber sorgfältige Datenqualität und Umschulungen, um negative Auswirkungen auf Arbeitsplätze zu minimieren.\n",
            "\n",
            "[TR]\n",
            "Yapay zeka, veri analizini hızlandırarak ve tekrarlayan işleri otomatikleştirerek birçok sektörde dönüşüm yaratırken, veri kalitesi ve dikkatli değerlendirme gibi faktörler dikkate alınması gereken önemli zorluklar taşır.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Change these values to review different cases\n",
        "PROMPT_ID = 1\n",
        "MODEL_ID = \"gemma3:1b\"  # or \"llama3.2:1b\"\n",
        "RUN_ID = 1\n",
        "\n",
        "display_full_comparison(PROMPT_ID, MODEL_ID, RUN_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Qualitative Evidence Set\n",
        "\n",
        "Generate a structured set of flagged examples for the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qualitative Evidence Set (fill in drift_type column manually):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>model_id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>task_type</th>\n",
              "      <th>pair</th>\n",
              "      <th>similarity</th>\n",
              "      <th>drift_type</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>open_text</td>\n",
              "      <td>phi3:latest</td>\n",
              "      <td>19</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>0.240370</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>open_text</td>\n",
              "      <td>llama3.2:1b</td>\n",
              "      <td>20</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>0.285214</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>open_text</td>\n",
              "      <td>phi3:latest</td>\n",
              "      <td>18</td>\n",
              "      <td>creative</td>\n",
              "      <td>EN-TR</td>\n",
              "      <td>0.297483</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>discrete</td>\n",
              "      <td>gemma3:4b</td>\n",
              "      <td>11</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>EN-DE-TR</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>EN=1, DE=78, TR=1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>discrete</td>\n",
              "      <td>gemma3:4b</td>\n",
              "      <td>11</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>EN-DE-TR</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>EN=1, DE=78, TR=1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>discrete</td>\n",
              "      <td>gemma3:1b</td>\n",
              "      <td>10</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>EN-DE-TR</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>EN=22.5, DE=60, TR=18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        type     model_id  prompt_id  task_type      pair  similarity  \\\n",
              "0  open_text  phi3:latest         19   creative     EN-TR    0.240370   \n",
              "1  open_text  llama3.2:1b         20   creative     EN-TR    0.285214   \n",
              "2  open_text  phi3:latest         18   creative     EN-TR    0.297483   \n",
              "3   discrete    gemma3:4b         11  reasoning  EN-DE-TR         NaN   \n",
              "4   discrete    gemma3:4b         11  reasoning  EN-DE-TR         NaN   \n",
              "5   discrete    gemma3:1b         10  reasoning  EN-DE-TR         NaN   \n",
              "\n",
              "  drift_type                  notes  \n",
              "0                                    \n",
              "1                                    \n",
              "2                                    \n",
              "3                 EN=1, DE=78, TR=1  \n",
              "4                 EN=1, DE=78, TR=1  \n",
              "5             EN=22.5, DE=60, TR=18  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create qualitative evidence set\n",
        "evidence_rows = []\n",
        "\n",
        "# Add open-text flagged examples (top 3 lowest)\n",
        "if len(flagged) > 0:\n",
        "    for _, row in flagged.head(3).iterrows():\n",
        "        evidence_rows.append({\n",
        "            'type': 'open_text',\n",
        "            'model_id': row['model_id'],\n",
        "            'prompt_id': row['prompt_id'],\n",
        "            'task_type': row['task_type'],\n",
        "            'pair': row['pair'],\n",
        "            'similarity': row['cosine_similarity'],\n",
        "            'drift_type': '',  # To be filled manually\n",
        "            'notes': ''  # To be filled manually\n",
        "        })\n",
        "\n",
        "# Add discrete mismatched examples (top 3)\n",
        "if len(mismatched) > 0:\n",
        "    for _, row in mismatched.head(3).iterrows():\n",
        "        evidence_rows.append({\n",
        "            'type': 'discrete',\n",
        "            'model_id': row['model_id'],\n",
        "            'prompt_id': row['prompt_id'],\n",
        "            'task_type': row['task_type'],\n",
        "            'pair': 'EN-DE-TR',\n",
        "            'similarity': None,\n",
        "            'drift_type': '',  # To be filled manually\n",
        "            'notes': f\"EN={row['key_en']}, DE={row['key_de']}, TR={row['key_tr']}\"\n",
        "        })\n",
        "\n",
        "evidence_df = pd.DataFrame(evidence_rows)\n",
        "print(\"Qualitative Evidence Set (fill in drift_type column manually):\")\n",
        "display(evidence_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to ../outputs/reports/qualitative_evidence.csv\n"
          ]
        }
      ],
      "source": [
        "# Save evidence set\n",
        "output_path = Path('../outputs/reports/qualitative_evidence.csv')\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "evidence_df.to_csv(output_path, index=False)\n",
        "print(f\"Saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drift Type Categories\n",
        "\n",
        "When annotating examples, use these categories:\n",
        "\n",
        "1. **Semantic Drift**: Core meaning differs (e.g., different conclusions, missing key points)\n",
        "2. **Format Drift**: Output format differs (e.g., bullet points vs prose, different length)\n",
        "3. **Factual Drift**: Factual content differs (e.g., wrong answer, different label)\n",
        "4. **Style Drift**: Tone or style differs significantly\n",
        "5. **Hallucination**: One language includes fabricated content\n",
        "\n",
        "---\n",
        "\n",
        "**End of Qualitative Review Notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-crossslingual-prompt-consistency",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
